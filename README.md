# NN-text-classification
Using Neural Network to classify movie reviews as positive or negative, based on the text of the review.
https://www.tensorflow.org/tutorials/keras/text_classification
https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory
https://stackoverflow.com/questions/997797/what-does-s-mean-in-a-python-format-string
https://stackoverflow.com/questions/280435/escaping-regex-string
https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization
https://www.tensorflow.org/api_docs/python/tf/expand_dims
https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer#:~:text=A%20dense%20layer%20is%20just,activations%20of%20previous%20layer%20a.https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work
https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy
https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set

Next, you will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.

When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test.

Note: When using the validation_split and subset arguments, make sure to either specify a random seed, or to pass shuffle=False, 
so that the validation and training splits have no overlap.

Preparing the dataset for training consists of 3 steps: standardize, tokenize, and vectorize.

Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset. 

Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words, by splitting on whitespace).

Vectorization refers to converting tokens into numbers so they can be fed into a neural network. 

All of these tasks can be accomplished with preprocessing.TextVectorization layer.

The reviews contain various HTML tags like br. These tags will not be removed by the default standardizer in the TextVectorization layer (which converts text to lowecase 
and strips punctuation by default, but doesn't strip HTML). You will write a custom standardization function to remove the HTML.

#Parameters: text to be processed, The regular expression to match the input, The rewrite to be applied to the matched expresion.
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')

 
The from_logits=True attribute inform the loss function that the output values generated by the model are not normalized, a.k.a. logits. 

In other words, the softmax function has not been applied on them to produce a probability distribution.

The softmax function would be automatically applied on the output values by the loss function. 

Therefore, from_logits=True scenario does not make a difference with the scenario when you use from_logits=False (default) and a softmax activation function on last layer;
In other words, If softmax layer is not being added at the last layer then we need to have the from_logits=True to indicate the probabilities are not normalized 
